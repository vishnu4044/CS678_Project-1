{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraies used in this IPYNB file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#torch lib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "#basic lib\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#sklearn Lib \n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#transformer lib Autotokenizer\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from transformers import BertModel, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "#NLTK lib and pandas\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import language_tool_python \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-requisite Function to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuda memory\n",
    "use_cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "base_model = 'twitter-xlm-roberta-base-sentiment'\n",
    "\n",
    "#model selection \n",
    "model_list = ['bert-base-uncased', 'bert-base-multilingual-uncased', 'google/muril-base-cased', 'xlm-roberta-base',\n",
    "              'ai4bharat/indic-bert','cardiffnlp/twitter-xlm-roberta-base','cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
    "              'cardiffnlp/twitter-roberta-base', 'cardiffnlp/twitter-roberta-base-sentiment',\n",
    "              'cardiffnlp/twitter-roberta-base-hate', 'roberta-base']\n",
    "\n",
    "#model path \n",
    "model_path = 'mnt/saved_models/'\n",
    "\n",
    "#result are saaved in this location \n",
    "results_path = 'mnt/saved_results/'\n",
    "\n",
    "#Data augumentation \n",
    "class HateData(Dataset):\n",
    "    def __init__(self, data_path, split='train', lang='bengali', aug_prob=0.2, flip_prob=0.5):\n",
    "        self.split = split\n",
    "        self.data = pd.read_csv(data_path + split + lang + \".tsv\", sep='\\t', lineterminator='\\n') \n",
    "        if self.split == 'train':\n",
    "            self.label2data = {0:[], 1:[], 2:[]}\n",
    "            for i in tqdm(range(len(self.data))):\n",
    "                row = self.data.iloc[i]\n",
    "                self.label2data[row[label_idx]].append(row[text_idx])\n",
    "            self.aug_prob = aug_prob\n",
    "            self.flip_prob = flip_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        data = self.data.iloc[index]\n",
    "        labels = data[label_idx]\n",
    "        text = data[text_idx]\n",
    "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        input_ids = inputs['input_ids']\n",
    "        token_type_ids = np.zeros(MAX_SEQ_LEN)\n",
    "        attn_mask = inputs['attention_mask']\n",
    "        aug_text = text  \n",
    "        labels_aug = labels\n",
    "        \n",
    "        if self.split == 'train' and labels == 1:\n",
    "            if np.random.uniform() < self.aug_prob:\n",
    "                aug_text = np.random.choice(self.label2data[0])\n",
    "         \n",
    "                if np.random.uniform() < self.flip_prob:\n",
    "                    aug_text = aug_text + \" [SEP] \" + text\n",
    "                else:\n",
    "                    aug_text = text + \" [SEP] \" + aug_text \n",
    "            labels_aug = 1\n",
    "      \n",
    "        inputs_aug = tokenizer(aug_text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        input_ids_aug = inputs_aug['input_ids']\n",
    "        token_type_ids_aug = np.zeros(MAX_SEQ_LEN)\n",
    "        attn_mask_aug = inputs_aug['attention_mask']\n",
    "\n",
    "        input_ids = torch.tensor(np.vstack([input_ids, input_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        token_type_ids = torch.tensor(np.vstack([token_type_ids, token_type_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        attn_mask = torch.tensor(np.vstack([attn_mask, attn_mask_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        labels = torch.tensor(np.vstack([labels, labels_aug]), dtype=torch.long).view(2)\n",
    "\n",
    "        return input_ids, attn_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#data classifier \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        H1, H2, num_class = 768, 128, 3\n",
    "        self.bert = AutoModel.from_pretrained(model_list[model_choice])\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(H1, H2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H2, H2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H2, num_class)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):  \n",
    "        outputs = self.bert(input_ids, attn_mask)\n",
    "        cls_emb = outputs.pooler_output \n",
    "        logits = self.clf(cls_emb)\n",
    "        return logits\n",
    "    \n",
    "#evaluate fuction \n",
    "def evaluate(input_ids, attn_mask, token_type_ids, label, model, mode='train'):\n",
    "   \n",
    "    batch_size = input_ids.shape[0]\n",
    "    seq_len = input_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            label = label.to(device)\n",
    "        \n",
    "        logits = model(input_ids[:,0,:], attn_mask[:,0,:], token_type_ids[:,0,:])\n",
    "        loss = loss_fn(logits, label[:,0])\n",
    "        \n",
    "        if mode == 'train':\n",
    "            return float(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        return float(loss.item()), preds.cpu().numpy()\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested Model with 1000 samples for Hate Explain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [04:15<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.599637269705534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8643    0.8008    0.8313       477\n",
      "           1     0.3617    0.3542    0.3579       144\n",
      "           2     0.7890    0.8681    0.8266       379\n",
      "\n",
      "    accuracy                         0.7620      1000\n",
      "   macro avg     0.6716    0.6744    0.6720      1000\n",
      "weighted avg     0.7634    0.7620    0.7614      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"data/hatexplain/hx_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"label\"][i])\n",
    "\n",
    "test_sam['post']=sam1\n",
    "test_sam['label']=sam2\n",
    "test_sam.to_csv(\"hxsam_test.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Hx_robert_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym Replacement\n",
    "#### All the words in the sentences are replaced by their synonyms while they're not stopwords, when their length is more than 3 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [04:18<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.7526975889280438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8337    0.7463    0.7876       477\n",
      "           1     0.3577    0.3056    0.3296       144\n",
      "           2     0.7111    0.8443    0.7720       379\n",
      "\n",
      "    accuracy                         0.7200      1000\n",
      "   macro avg     0.6342    0.6321    0.6297      1000\n",
      "weighted avg     0.7187    0.7200    0.7157      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=pd.read_csv(\"data/hatexplain/hx_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"label\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    for i in l:\n",
    "        if i in stop_words:\n",
    "            sen.append(i)\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                if len(i)<=3:\n",
    "                    sen.append(i)\n",
    "                else:\n",
    "                    j=wordnet.synsets(i)\n",
    "                    lt=j[0].lemmas()[0].name()\n",
    "                    if i !=lt:\n",
    "                        sen.append(lt)\n",
    "                    else:\n",
    "                        sen.append(i)\n",
    "            except:\n",
    "                sen.append(i)\n",
    "    post.append(' '.join(sen))\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Hx_robert_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charcter swaping\n",
    "#### All the words in the sentences are swaped their charcters while they're not stopwords, when their length is more than 3 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:39<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.6691783783510328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8419    0.7925    0.8164       477\n",
      "           1     0.3356    0.3403    0.3379       144\n",
      "           2     0.7654    0.8179    0.7908       379\n",
      "\n",
      "    accuracy                         0.7370      1000\n",
      "   macro avg     0.6476    0.6502    0.6484      1000\n",
      "weighted avg     0.7400    0.7370    0.7378      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import language_tool_python \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "data=pd.read_csv(\"data/hatexplain/hx_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"label\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    count=0\n",
    "    for i in l:\n",
    "        if i in stop_words:\n",
    "            sen.append(i)\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                if len(i)<=3:\n",
    "                    sen.append(i)\n",
    "                else:\n",
    "                    if count==2:\n",
    "                        sen.append(i)\n",
    "                    else:\n",
    "                        t=list(i)\n",
    "                        t[2],t[3]=t[3],t[2]\n",
    "                        r=\"\".join(t)\n",
    "                        count=count+1\n",
    "                        sen.append(r)\n",
    "            except:\n",
    "                sen.append(i)\n",
    "    post.append(' '.join(sen))\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Hx_robert_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation last charcter of the word to next word in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:49<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.6058173116743565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8617    0.7966    0.8279       477\n",
      "           1     0.3624    0.3750    0.3686       144\n",
      "           2     0.7951    0.8602    0.8264       379\n",
      "\n",
      "    accuracy                         0.7600      1000\n",
      "   macro avg     0.6731    0.6773    0.6743      1000\n",
      "weighted avg     0.7646    0.7600    0.7612      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import language_tool_python \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "data=pd.read_csv(\"data/hatexplain/hx_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"label\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    count=0\n",
    "    word=\"\"\n",
    "    for i in l:\n",
    "        if word!=\"\":\n",
    "            i=word+i\n",
    "            word=\"\"\n",
    "        else:\n",
    "            pass\n",
    "        if i in stop_words:\n",
    "            sen.append(i)\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                if len(i)<=3:\n",
    "                    sen.append(i)\n",
    "                else:\n",
    "                    if count==1:\n",
    "                        sen.append(i)\n",
    "                    else:\n",
    "                        t=list(i)\n",
    "                        word=t.pop()\n",
    "                        #print(word)\n",
    "                        r=\"\".join(t)\n",
    "                        count=count+1\n",
    "                        sen.append(r)\n",
    "            except:\n",
    "                sen.append(i)\n",
    "    post.append(' '.join(sen))\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Hx_robert_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping few words from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [06:01<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.7552851028814912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7851    0.8197    0.8021       477\n",
      "           1     0.3376    0.3681    0.3522       144\n",
      "           2     0.7971    0.7256    0.7597       379\n",
      "\n",
      "    accuracy                         0.7190      1000\n",
      "   macro avg     0.6399    0.6378    0.6380      1000\n",
      "weighted avg     0.7252    0.7190    0.7212      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import language_tool_python \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "data=pd.read_csv(\"data/hatexplain/hx_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"label\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    count=0\n",
    "    word=\"\"\n",
    "    for i in l:\n",
    "        sen.append(i)\n",
    "    random.shuffle(sen)\n",
    "    sen.pop()\n",
    "    random.shuffle(sen)\n",
    "    sen.pop()\n",
    "    post.append(' '.join(sen))\n",
    "    post\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Hx_robert_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:47<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.634000144816935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8533    0.7925    0.8217       477\n",
      "           1     0.3245    0.3403    0.3322       144\n",
      "           2     0.7759    0.8311    0.8025       379\n",
      "\n",
      "    accuracy                         0.7420      1000\n",
      "   macro avg     0.6512    0.6546    0.6522      1000\n",
      "weighted avg     0.7478    0.7420    0.7440      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import language_tool_python \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "data=pd.read_csv(\"data/hatexplain/hx_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"label\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    count=0\n",
    "    word=\"\"\n",
    "    for i in l:\n",
    "        sen.append(i)\n",
    "    random.shuffle(sen)\n",
    "    random.shuffle(sen)\n",
    "    post.append(' '.join(sen))\n",
    "    post\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Hx_robert_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
