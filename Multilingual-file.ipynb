{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraies used in this IPYNB file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#torch lib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "#basic lib\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#sklearn Lib \n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#transformer lib Autotokenizer\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from transformers import BertModel, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "#NLTK lib and pandas\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import language_tool_python \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-requisite Function to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuda memory\n",
    "use_cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "base_model = 'twitter-xlm-roberta-base-sentiment'\n",
    "\n",
    "#model selection \n",
    "model_list = ['bert-base-uncased', 'bert-base-multilingual-uncased', 'google/muril-base-cased', 'xlm-roberta-base',\n",
    "              'ai4bharat/indic-bert','cardiffnlp/twitter-xlm-roberta-base','cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
    "              'cardiffnlp/twitter-roberta-base', 'cardiffnlp/twitter-roberta-base-sentiment',\n",
    "              'cardiffnlp/twitter-roberta-base-hate', 'roberta-base']\n",
    "\n",
    "#model path \n",
    "model_path = 'mnt/saved_models/'\n",
    "\n",
    "#result are saaved in this location \n",
    "results_path = 'mnt/saved_results/'\n",
    "\n",
    "#Data augumentation \n",
    "class HateData(Dataset):\n",
    "    def __init__(self, data_path, split='train', lang='bengali', aug_prob=0.2, flip_prob=0.5):\n",
    "        self.split = split\n",
    "        self.data = pd.read_csv(data_path + split + lang , sep=',', lineterminator='\\n') \n",
    "        if self.split == 'train':\n",
    "            self.label2data = {0:[], 1:[], 2:[]}\n",
    "            for i in tqdm(range(len(self.data))):\n",
    "                row = self.data.iloc[i]\n",
    "                self.label2data[row[label_idx]].append(row[text_idx])\n",
    "            self.aug_prob = aug_prob\n",
    "            self.flip_prob = flip_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        data = self.data.iloc[index]\n",
    "        labels = data[label_idx]\n",
    "        text = data[text_idx]\n",
    "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        input_ids = inputs['input_ids']\n",
    "        token_type_ids = np.zeros(MAX_SEQ_LEN)\n",
    "        attn_mask = inputs['attention_mask']\n",
    "        aug_text = text  \n",
    "        labels_aug = labels\n",
    "        \n",
    "        if self.split == 'train' and labels == 1:\n",
    "            if np.random.uniform() < self.aug_prob:\n",
    "                aug_text = np.random.choice(self.label2data[0])\n",
    "         \n",
    "                if np.random.uniform() < self.flip_prob:\n",
    "                    aug_text = aug_text + \" [SEP] \" + text\n",
    "                else:\n",
    "                    aug_text = text + \" [SEP] \" + aug_text \n",
    "            labels_aug = 1\n",
    "      \n",
    "        inputs_aug = tokenizer(aug_text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        input_ids_aug = inputs_aug['input_ids']\n",
    "        token_type_ids_aug = np.zeros(MAX_SEQ_LEN)\n",
    "        attn_mask_aug = inputs_aug['attention_mask']\n",
    "\n",
    "        input_ids = torch.tensor(np.vstack([input_ids, input_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        token_type_ids = torch.tensor(np.vstack([token_type_ids, token_type_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        attn_mask = torch.tensor(np.vstack([attn_mask, attn_mask_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        labels = torch.tensor(np.vstack([labels, labels_aug]), dtype=torch.long).view(2)\n",
    "\n",
    "        return input_ids, attn_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#data classifier \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        H1, H2, num_class = 768, 128, 2\n",
    "        self.bert = AutoModel.from_pretrained(model_list[model_choice])\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(H1, H2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H2, H2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H2, num_class)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):  \n",
    "        outputs = self.bert(input_ids, attn_mask)\n",
    "        cls_emb = outputs.pooler_output \n",
    "        logits = self.clf(cls_emb)\n",
    "        return logits\n",
    "    \n",
    "#evaluate fuction \n",
    "def evaluate(input_ids, attn_mask, token_type_ids, label, model, mode='train'):\n",
    "   \n",
    "    batch_size = input_ids.shape[0]\n",
    "    seq_len = input_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            label = label.to(device)\n",
    "        \n",
    "        logits = model(input_ids[:,0,:], attn_mask[:,0,:], token_type_ids[:,0,:])\n",
    "        loss = loss_fn(logits, label[:,0])\n",
    "        \n",
    "        if mode == 'train':\n",
    "            return float(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        return float(loss.item()), preds.cpu().numpy()\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-uncased\n",
      "\n",
      "\n",
      "Language \n",
      "\n",
      "japaness\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:17<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.4407794945336019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8980    0.9778    0.9362        45\n",
      "           1     0.6667    0.2857    0.4000         7\n",
      "\n",
      "    accuracy                         0.8846        52\n",
      "   macro avg     0.7823    0.6317    0.6681        52\n",
      "weighted avg     0.8668    0.8846    0.8640        52\n",
      "\n",
      "Error Analysis for japaness\n",
      "フランスをイスラム化する極悪非道な計画を実行しているマクロン大統領が今何をしているのかがこれだ、水起きろ… @url\n",
      "@user 彼がアラブ人に変装していようが、黒人がユダヤ人に変装していようが、私には関係ありません。しかし、そこには変装が… @url\n",
      "真実 #イスラム教 #イスラム教徒 #モスク #検閲 #sos #ヨーロッパ #自由表現 #歌 #左翼… @url\n",
      "まあ、いいえ、でもレベウスにも巨根はあります — 良い tg @url\n",
      "@user 強制的に遅刻者はくそー\n",
      "Language \n",
      "\n",
      "italin\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:16<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.5404823472938285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8776    0.9556    0.9149        45\n",
      "           1     0.3333    0.1429    0.2000         7\n",
      "\n",
      "    accuracy                         0.8462        52\n",
      "   macro avg     0.6054    0.5492    0.5574        52\n",
      "weighted avg     0.8043    0.8462    0.8187        52\n",
      "\n",
      "Error Analysis for italin\n",
      "ecco cosa sta facendo Macron nel momento in cui sta attuando un piano diabolico per islamizzare la Francia, svegliati... @url\n",
      "@user se è travestito da arabo o nero da ebreo o altro, non mi interessa. ma ecco il travestimento… @url\n",
      "verità #islam #musulmani #moschea #censura #sos #Europa #espressionedellalibertà #ong #sinistra… @url\n",
      "beh no, ma anche i rebeus hanno un grosso cazzo — bravo tg @url\n",
      "@usate tutte queste persone che rifiutano di cedere al terrorismo intellettuale che deve spaventare più di una persona.\n",
      "Language \n",
      "\n",
      "russian\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:16<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.44519931620631653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9149    0.9556    0.9348        45\n",
      "           1     0.6000    0.4286    0.5000         7\n",
      "\n",
      "    accuracy                         0.8846        52\n",
      "   macro avg     0.7574    0.6921    0.7174        52\n",
      "weighted avg     0.8725    0.8846    0.8763        52\n",
      "\n",
      "Error Analysis for russian\n",
      "iată ce face Macron în momentul în care implementează un plan diabolic de islamizare a Franței, trezește-te miercuri... @url\n",
      "truth #islam #musulmani #moschee #cenzură #sos #europe #freedomexpression #ong #leftist… @url\n",
      "@user al naibii de retardații în vigoare\n",
      "Marea Britanie: rugăciunea musulmană este recitată (în arabă) înainte de deschiderea ședinței consiliului municipal din... @url\n",
      "- expulzarea imediată a migranților vinovați de furt viol...\n",
      "Language \n",
      "\n",
      "romanian\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:15<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.5024353414594841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8776    0.9556    0.9149        45\n",
      "           1     0.3333    0.1429    0.2000         7\n",
      "\n",
      "    accuracy                         0.8462        52\n",
      "   macro avg     0.6054    0.5492    0.5574        52\n",
      "weighted avg     0.8043    0.8462    0.8187        52\n",
      "\n",
      "Error Analysis for romanian\n",
      "вот что делает Макрон в тот момент, когда реализует дьявольский план по исламизации Франции, проснись, ср… @url\n",
      "истина #ислам #мусульмане #мечеть #цензура #сос #европа #свободавыражение #онг #левый… @url\n",
      "ну нет, но у ребеуса тоже большой член — хорошо, tg @url\n",
      "@используйте всех этих людей, которые отказываются поддаваться интеллектуальному терроризму, который должен напугать не одного человека.\n",
      "@user черт возьми, какие дебилы в силе\n",
      "xlm-roberta-base\n",
      "\n",
      "\n",
      "Language \n",
      "\n",
      "japaness\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:16<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.3052644353861419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9149    0.9556    0.9348        45\n",
      "           1     0.6000    0.4286    0.5000         7\n",
      "\n",
      "    accuracy                         0.8846        52\n",
      "   macro avg     0.7574    0.6921    0.7174        52\n",
      "weighted avg     0.8725    0.8846    0.8763        52\n",
      "\n",
      "Error Analysis for japaness\n",
      "真実 #イスラム教 #イスラム教徒 #モスク #検閲 #sos #ヨーロッパ #自由表現 #歌 #左翼… @url\n",
      "まあ、いいえ、でもレベウスにも巨根はあります — 良い tg @url\n",
      "@user 強制的に遅刻者はくそー\n",
      "彼女は汚いアラブ人だった @url\n",
      "イギリス：市議会の開会前にイスラム教徒の祈りが（アラビア語で）唱えられる… @url\n",
      "Language \n",
      "\n",
      "italin\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:16<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.2896663761482789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8936    0.9333    0.9130        45\n",
      "           1     0.4000    0.2857    0.3333         7\n",
      "\n",
      "    accuracy                         0.8462        52\n",
      "   macro avg     0.6468    0.6095    0.6232        52\n",
      "weighted avg     0.8272    0.8462    0.8350        52\n",
      "\n",
      "Error Analysis for italin\n",
      "ecco cosa sta facendo Macron nel momento in cui sta attuando un piano diabolico per islamizzare la Francia, svegliati... @url\n",
      "dc ora dicono che @user o l'@user sono una lobby ebraica e sionista sapendo che pretende di essere tale… @url\n",
      "verità #islam #musulmani #moschea #censura #sos #Europa #espressionedellalibertà #ong #sinistra… @url\n",
      "beh no, ma anche i rebeus hanno un grosso cazzo — bravo tg @url\n",
      "@user maledetti i ritardati in vigore\n",
      "Language \n",
      "\n",
      "russian\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:15<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.30872314548693025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8913    0.9111    0.9011        45\n",
      "           1     0.3333    0.2857    0.3077         7\n",
      "\n",
      "    accuracy                         0.8269        52\n",
      "   macro avg     0.6123    0.5984    0.6044        52\n",
      "weighted avg     0.8162    0.8269    0.8212        52\n",
      "\n",
      "Error Analysis for russian\n",
      "iată ce face Macron în momentul în care implementează un plan diabolic de islamizare a Franței, trezește-te miercuri... @url\n",
      "acesta este rezultatul raportului despre islamism care confundă islamul cu terorismul.\n",
      "truth #islam #musulmani #moschee #cenzură #sos #europe #freedomexpression #ong #leftist… @url\n",
      "ei bine, nu, dar și rebeus au un cocoș mare — bun tg @url\n",
      "este chiar acolo te arăți sugând rebeusul nu-ți spun, dar îți spun că dacă aș fi în locul tău mi-ar fi rușine — m… @url\n",
      "Language \n",
      "\n",
      "romanian\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:15<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.31877099998438585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8696    0.8889    0.8791        45\n",
      "           1     0.1667    0.1429    0.1538         7\n",
      "\n",
      "    accuracy                         0.7885        52\n",
      "   macro avg     0.5181    0.5159    0.5165        52\n",
      "weighted avg     0.7749    0.7885    0.7815        52\n",
      "\n",
      "Error Analysis for romanian\n",
      "Со мной что-то подобное происходит, я разгоняюсь как монгол и наезжаю на людей, я сошел с ума (я плачу на гонках в реальной жизни) @url\n",
      "вот что делает Макрон в тот момент, когда реализует дьявольский план по исламизации Франции, проснись, ср… @url\n",
      "это результат доклада об исламизме, который путает ислам и терроризм.\n",
      "dc теперь говорят, что @user или @user являются еврейским и сионистским лобби, зная, что оно претендует на таковое… @url\n",
      "истина #ислам #мусульмане #мечеть #цензура #сос #европа #свободавыражение #онг #левый… @url\n",
      "cardiffnlp/twitter-xlm-roberta-base\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language \n",
      "\n",
      "japaness\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:19<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.5535262277967726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8958    0.9556    0.9247        45\n",
      "           1     0.5000    0.2857    0.3636         7\n",
      "\n",
      "    accuracy                         0.8654        52\n",
      "   macro avg     0.6979    0.6206    0.6442        52\n",
      "weighted avg     0.8425    0.8654    0.8492        52\n",
      "\n",
      "Error Analysis for japaness\n",
      "フランスをイスラム化する極悪非道な計画を実行しているマクロン大統領が今何をしているのかがこれだ、水起きろ… @url\n",
      "まあ、いいえ、でもレベウスにも巨根はあります — 良い tg @url\n",
      "@複数の人を怖がらせる知的テロリズムに屈することを拒否するこれらの人々をすべて利用してください。\n",
      "@user 強制的に遅刻者はくそー\n",
      "イギリス：市議会の開会前にイスラム教徒の祈りが（アラビア語で）唱えられる… @url\n",
      "Language \n",
      "\n",
      "italin\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:16<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.9690257287046944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8571    0.9333    0.8936        45\n",
      "           1     0.0000    0.0000    0.0000         7\n",
      "\n",
      "    accuracy                         0.8077        52\n",
      "   macro avg     0.4286    0.4667    0.4468        52\n",
      "weighted avg     0.7418    0.8077    0.7733        52\n",
      "\n",
      "Error Analysis for italin\n",
      "ecco cosa sta facendo Macron nel momento in cui sta attuando un piano diabolico per islamizzare la Francia, svegliati... @url\n",
      "@user se è travestito da arabo o nero da ebreo o altro, non mi interessa. ma ecco il travestimento… @url\n",
      "verità #islam #musulmani #moschea #censura #sos #Europa #espressionedellalibertà #ong #sinistra… @url\n",
      "beh no, ma anche i rebeus hanno un grosso cazzo — bravo tg @url\n",
      "@usate tutte queste persone che rifiutano di cedere al terrorismo intellettuale che deve spaventare più di una persona.\n",
      "Language \n",
      "\n",
      "russian\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:16<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.8372328490636741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8627    0.9778    0.9167        45\n",
      "           1     0.0000    0.0000    0.0000         7\n",
      "\n",
      "    accuracy                         0.8462        52\n",
      "   macro avg     0.4314    0.4889    0.4583        52\n",
      "weighted avg     0.7466    0.8462    0.7933        52\n",
      "\n",
      "Error Analysis for russian\n",
      "iată ce face Macron în momentul în care implementează un plan diabolic de islamizare a Franței, trezește-te miercuri... @url\n",
      "@utilizator indiferent dacă este deghizat în arab sau negru ca evreu sau orice altceva, nu-mi pasă. dar acolo deghizarea... @url\n",
      "ei bine, nu, dar și rebeus au un cocoș mare — bun tg @url\n",
      "@user al naibii de retardații în vigoare\n",
      "Marea Britanie: rugăciunea musulmană este recitată (în arabă) înainte de deschiderea ședinței consiliului municipal din... @url\n",
      "Language \n",
      "\n",
      "romanian\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:16<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.7660414250567555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8776    0.9556    0.9149        45\n",
      "           1     0.3333    0.1429    0.2000         7\n",
      "\n",
      "    accuracy                         0.8462        52\n",
      "   macro avg     0.6054    0.5492    0.5574        52\n",
      "weighted avg     0.8043    0.8462    0.8187        52\n",
      "\n",
      "Error Analysis for romanian\n",
      "вот что делает Макрон в тот момент, когда реализует дьявольский план по исламизации Франции, проснись, ср… @url\n",
      "@user, меня не волнует, замаскирован ли он под араба или чернокожего под еврея или что-то в этом роде. но там маскировка… @url\n",
      "ну нет, но у ребеуса тоже большой член — хорошо, tg @url\n",
      "@используйте всех этих людей, которые отказываются поддаваться интеллектуальному терроризму, который должен напугать не одного человека.\n",
      "@user черт возьми, какие дебилы в силе\n",
      "cardiffnlp/twitter-xlm-roberta-base-sentiment\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language \n",
      "\n",
      "japaness\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:17<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.49390753698893464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8776    0.9556    0.9149        45\n",
      "           1     0.3333    0.1429    0.2000         7\n",
      "\n",
      "    accuracy                         0.8462        52\n",
      "   macro avg     0.6054    0.5492    0.5574        52\n",
      "weighted avg     0.8043    0.8462    0.8187        52\n",
      "\n",
      "Error Analysis for japaness\n",
      "フランスをイスラム化する極悪非道な計画を実行しているマクロン大統領が今何をしているのかがこれだ、水起きろ… @url\n",
      "@user 彼がアラブ人に変装していようが、黒人がユダヤ人に変装していようが、私には関係ありません。しかし、そこには変装が… @url\n",
      "まあ、いいえ、でもレベウスにも巨根はあります — 良い tg @url\n",
      "@複数の人を怖がらせる知的テロリズムに屈することを拒否するこれらの人々をすべて利用してください。\n",
      "@user 強制的に遅刻者はくそー\n",
      "Language \n",
      "\n",
      "italin\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:17<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.41751978711153453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8800    0.9778    0.9263        45\n",
      "           1     0.5000    0.1429    0.2222         7\n",
      "\n",
      "    accuracy                         0.8654        52\n",
      "   macro avg     0.6900    0.5603    0.5743        52\n",
      "weighted avg     0.8288    0.8654    0.8315        52\n",
      "\n",
      "Error Analysis for italin\n",
      "ecco cosa sta facendo Macron nel momento in cui sta attuando un piano diabolico per islamizzare la Francia, svegliati... @url\n",
      "@user se è travestito da arabo o nero da ebreo o altro, non mi interessa. ma ecco il travestimento… @url\n",
      "@usate tutte queste persone che rifiutano di cedere al terrorismo intellettuale che deve spaventare più di una persona.\n",
      "@user maledetti i ritardati in vigore\n",
      "Gran Bretagna: si recita la preghiera musulmana (in arabo) prima dell'apertura della seduta del consiglio comunale di… @url\n",
      "Language \n",
      "\n",
      "russian\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:17<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.6571767923350518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8636    0.8444    0.8539        45\n",
      "           1     0.1250    0.1429    0.1333         7\n",
      "\n",
      "    accuracy                         0.7500        52\n",
      "   macro avg     0.4943    0.4937    0.4936        52\n",
      "weighted avg     0.7642    0.7500    0.7569        52\n",
      "\n",
      "Error Analysis for russian\n",
      "opozitia nu are program si se opreste pe dezbateri false matar ba ministru @url\n",
      "iată ce face Macron în momentul în care implementează un plan diabolic de islamizare a Franței, trezește-te miercuri... @url\n",
      "@utilizator indiferent dacă este deghizat în arab sau negru ca evreu sau orice altceva, nu-mi pasă. dar acolo deghizarea... @url\n",
      "rt @user istoria Quebecului merită să-i acordăm atenție. atât de trist încât cu greu o mai predăm. text foarte interesant\n",
      "@folosește pe toți acești oameni care refuză să cedeze terorismului intelectual care trebuie să sperie mai mult de o persoană.\n",
      "Language \n",
      "\n",
      "romanian\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:17<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.4352676968734998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8958    0.9556    0.9247        45\n",
      "           1     0.5000    0.2857    0.3636         7\n",
      "\n",
      "    accuracy                         0.8654        52\n",
      "   macro avg     0.6979    0.6206    0.6442        52\n",
      "weighted avg     0.8425    0.8654    0.8492        52\n",
      "\n",
      "Error Analysis for romanian\n",
      "вот что делает Макрон в тот момент, когда реализует дьявольский план по исламизации Франции, проснись, ср… @url\n",
      "@user, меня не волнует, замаскирован ли он под араба или чернокожего под еврея или что-то в этом роде. но там маскировка… @url\n",
      "ну нет, но у ребеуса тоже большой член — хорошо, tg @url\n",
      "@используйте всех этих людей, которые отказываются поддаваться интеллектуальному терроризму, который должен напугать не одного человека.\n",
      "она была грязной арабкой @url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_names=[\"all-language-bert-base-multilingual\",\"all_language_xlm-roberta-base\",\"all_language_twitter-xlm-roberta-base\",\"all_language_twitter-xlm-roberta-base-sentiment\"]\n",
    "model_num=[1,3,5,6]\n",
    "#Load pre trained Model\n",
    "for j in range(4):\n",
    "    model_choice=model_num[j]\n",
    "    print(model_list[model_num[j]])\n",
    "    print(\"\\n\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_list[model_num[j]])\n",
    "    model = Classifier()\n",
    "    model.load_state_dict(torch.load(\"mnt/saved_models/\"+model_names[j]+\".pth\", map_location=device))\n",
    "    model = model.to(device)\n",
    "    label_idx = 1\n",
    "    MAX_SEQ_LEN = 128\n",
    "    text_idx = 0\n",
    "    lan=['ja','it','ro','ru']\n",
    "    name=['japaness', 'italin','russian','romanian']\n",
    "    for i in range(4):\n",
    "        print(\"Language \\n\")\n",
    "        print(name[i])\n",
    "        print(\"\\n\")\n",
    "        test_data = HateData(data_path=\"data/multilingual/2/\"+lan[i]+\"_test.tsv\", split='', lang=\"\")\n",
    "        test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        test_pred = []\n",
    "\n",
    "        #Record the prediction result  \n",
    "        wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "        for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "            v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "            test_loss.append(v_loss)\n",
    "            test_pred.append(v_pred)\n",
    "            wr.write(str(v_pred)+\"\\n\")\n",
    "        test_loss = np.mean(test_loss)#.item()\n",
    "        print(\"Test Loss: \", test_loss)\n",
    "        wr.close()\n",
    "\n",
    "\n",
    "        df_test = pd.read_csv(\"data/multilingual/2/\"+lan[i]+\"_test.tsv\", sep=',', lineterminator='\\n')\n",
    "        gt_labels = df_test[\"label\"]\n",
    "\n",
    "        print(classification_report(gt_labels, test_pred, digits=4))\n",
    "\n",
    "        print(\"Error Analysis for \"+name[i])\n",
    "        count=0\n",
    "        for i in range(50):\n",
    "            if gt_labels[i]!=test_pred[i]:\n",
    "                print(df_test['post'][i])\n",
    "                count=count+1\n",
    "                if count>=5:\n",
    "                    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
