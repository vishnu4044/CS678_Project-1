{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "39ea1973-e66a-44d8-843b-d67a364157e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ea1973-e66a-44d8-843b-d67a364157e2",
        "outputId": "67f2d51c-c7d8-4ba5-d30a-319cd6b7a335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "#Lib to be installed before running the code\n",
        "\n",
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install tensorboard\n",
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9d872596",
      "metadata": {
        "id": "9d872596"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ad239020-27bb-4886-8c61-e1908e57e506",
      "metadata": {
        "id": "ad239020-27bb-4886-8c61-e1908e57e506"
      },
      "outputs": [],
      "source": [
        "#math calculation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#torch lib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch import optim\n",
        "\n",
        "#basic lib\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "#sklearn Lib\n",
        "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#transformer lib Autotokenizer\n",
        "from transformers import BertTokenizer, AutoTokenizer\n",
        "from transformers import BertModel, AutoModel, AutoModelForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "#Cuda memory\n",
        "use_cuda = True if torch.cuda.is_available() else False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "base_model = 'twitter-xlm-roberta-base-sentiment'\n",
        "model_list = ['bert-base-uncased', 'bert-base-multilingual-uncased', 'google/muril-base-cased', 'xlm-roberta-base',\n",
        "              'ai4bharat/indic-bert','cardiffnlp/twitter-xlm-roberta-base','cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
        "              'cardiffnlp/twitter-roberta-base', 'cardiffnlp/twitter-roberta-base-sentiment',\n",
        "              'cardiffnlp/twitter-roberta-base-hate', 'roberta-base']\n",
        "model_path = 'mnt/saved_models/'\n",
        "results_path = 'mnt/saved_results/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0da5f88b-0f84-4c67-8eba-8a90b0095c5a",
      "metadata": {
        "id": "0da5f88b-0f84-4c67-8eba-8a90b0095c5a"
      },
      "outputs": [],
      "source": [
        "#to set up the dataset to train and test\n",
        "lang = 'hx'\n",
        "model_choice = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "99050998-e6fc-4236-b022-6b337c2aad28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99050998-e6fc-4236-b022-6b337c2aad28",
        "outputId": "475c849a-297e-4fc4-ae40-771db5a4258e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "writer = SummaryWriter(log_dir=\"/home/jupyter/tboard/\" + base_model + \"_\" + lang)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9f2fcfaf-e554-4934-9494-4e162e6d8fc4",
      "metadata": {
        "id": "9f2fcfaf-e554-4934-9494-4e162e6d8fc4"
      },
      "outputs": [],
      "source": [
        "#to set pre trained tokenizer for the given dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
        "\n",
        "#max sequence Length\n",
        "MAX_SEQ_LEN = 128\n",
        "\n",
        "label_idx = 1\n",
        "text_idx = 0\n",
        "\n",
        "#to read data and augument the data we have this function\n",
        "class HateData(Dataset):\n",
        "    def __init__(self, data_path, split='train', lang='bengali', aug_prob=0.2, flip_prob=0.5,sep='\\t'):\n",
        "        self.split = split\n",
        "        self.data = pd.read_csv(data_path+ lang +'_'+split+ \".tsv\", sep=sep, lineterminator='\\n')\n",
        "        if self.split == 'train':\n",
        "            self.label2data = {0:[], 1:[], 2:[]}\n",
        "            for i in tqdm(range(len(self.data))):\n",
        "                row = self.data.iloc[i]\n",
        "                self.label2data[row[label_idx]].append(row[text_idx])\n",
        "            self.aug_prob = aug_prob\n",
        "            self.flip_prob = flip_prob\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        data = self.data.iloc[index]\n",
        "        labels = data[label_idx]\n",
        "        text = data[text_idx]\n",
        "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
        "        input_ids = inputs['input_ids']\n",
        "        token_type_ids = np.zeros(MAX_SEQ_LEN)\n",
        "        attn_mask = inputs['attention_mask']\n",
        "        aug_text = text\n",
        "        labels_aug = labels\n",
        "\n",
        "        if self.split == 'train' and labels == 1:\n",
        "            if np.random.uniform() < self.aug_prob:\n",
        "                aug_text = np.random.choice(self.label2data[0])\n",
        "\n",
        "                if np.random.uniform() < self.flip_prob:\n",
        "                    aug_text = aug_text + \" [SEP] \" + text\n",
        "                else:\n",
        "                    aug_text = text + \" [SEP] \" + aug_text\n",
        "            labels_aug = 1\n",
        "\n",
        "        inputs_aug = tokenizer(aug_text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
        "        input_ids_aug = inputs_aug['input_ids']\n",
        "        token_type_ids_aug = np.zeros(MAX_SEQ_LEN)\n",
        "        attn_mask_aug = inputs_aug['attention_mask']\n",
        "\n",
        "        input_ids = torch.tensor(np.vstack([input_ids, input_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
        "        token_type_ids = torch.tensor(np.vstack([token_type_ids, token_type_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
        "        attn_mask = torch.tensor(np.vstack([attn_mask, attn_mask_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
        "        labels = torch.tensor(np.vstack([labels, labels_aug]), dtype=torch.long).view(2)\n",
        "\n",
        "        return input_ids, attn_mask, token_type_ids, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cf8548d8-c91b-4ccf-a518-0635f05579ef",
      "metadata": {
        "id": "cf8548d8-c91b-4ccf-a518-0635f05579ef"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        H1, H2, num_class = 768, 128, 3\n",
        "        self.bert = AutoModel.from_pretrained(model_list[model_choice])\n",
        "        self.clf = nn.Sequential(\n",
        "            nn.Linear(H1, H2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H2, H2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H2, num_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        outputs = self.bert(input_ids, attn_mask)\n",
        "        cls_emb = outputs.pooler_output\n",
        "        logits = self.clf(cls_emb)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b9bbaa46-9b44-49af-bb49-124e9af439ba",
      "metadata": {
        "id": "b9bbaa46-9b44-49af-bb49-124e9af439ba"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9a4d0548-5e3d-4279-ad0b-1d9d566d3f90",
      "metadata": {
        "id": "9a4d0548-5e3d-4279-ad0b-1d9d566d3f90"
      },
      "outputs": [],
      "source": [
        "\n",
        "#We used this function to train the data\n",
        "\n",
        "def train(input_ids, attn_mask, token_type_ids, label, model, model_opt, scdl):\n",
        "    model_opt.zero_grad()\n",
        "    batch_size = input_ids.shape[0]\n",
        "    seq_len = input_ids.shape[1]\n",
        "    loss = 0.0\n",
        "    if use_cuda:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attn_mask = attn_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        label = label.to(device)\n",
        "    logits = model(input_ids[:,0,:], attn_mask[:,0,:], token_type_ids[:,0,:])\n",
        "    logits_aug = model(input_ids[:,1,:], attn_mask[:,1,:], token_type_ids[:,1,:])\n",
        "    loss = loss_fn(logits, label[:,0]) + loss_fn(logits_aug, label[:,1])\n",
        "    loss.backward()\n",
        "    model_opt.step()\n",
        "    scdl.step()\n",
        "    return float(loss.item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9ad57855-ee8b-448e-9c03-0049d81db476",
      "metadata": {
        "id": "9ad57855-ee8b-448e-9c03-0049d81db476"
      },
      "outputs": [],
      "source": [
        "\n",
        "#evaluate fuction\n",
        "def evaluate(input_ids, attn_mask, token_type_ids, label, model, mode='train'):\n",
        "\n",
        "    batch_size = input_ids.shape[0]\n",
        "    seq_len = input_ids.shape[1]\n",
        "    with torch.no_grad():\n",
        "        if use_cuda:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attn_mask = attn_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "        logits = model(input_ids[:,0,:], attn_mask[:,0,:], token_type_ids[:,0,:])\n",
        "        loss = loss_fn(logits, label[:,0])\n",
        "\n",
        "        if mode == 'train':\n",
        "            return float(loss.item())\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        return float(loss.item()), preds.cpu().numpy()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1958148f-3d6f-4574-b799-9edf2a791f25",
      "metadata": {
        "id": "1958148f-3d6f-4574-b799-9edf2a791f25"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(\"hx_test.tsv\", sep='\\t', lineterminator='\\n')\n",
        "gt_labels = np.array(df_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f7016425-389c-45d9-8cb6-a11c73fcdb1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7016425-389c-45d9-8cb6-a11c73fcdb1f",
        "outputId": "8bed9d48-c2b6-4799-f538-6e08af509d98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1924"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(gt_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dabe9a9f-e8a6-4712-bf00-ae5d75d16bd5",
      "metadata": {
        "id": "dabe9a9f-e8a6-4712-bf00-ae5d75d16bd5"
      },
      "outputs": [],
      "source": [
        "#save the model and  train the data\n",
        "def trainIters(model, epochs, train_loader, test_loader, learning_rate=3e-5, log_step=168, valid_step=168, mode='train'):\n",
        "\n",
        "    model_opt = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "    num_train_steps = (len(train_loader)*epochs)\n",
        "    scdl = get_linear_schedule_with_warmup(model_opt, num_warmup_steps=int(0.1*num_train_steps), num_training_steps=num_train_steps)\n",
        "\n",
        "    print(\"Initialised optimizer and lr scheduler\")\n",
        "\n",
        "    # valid_best_loss = []\n",
        "    best_acc = 0.0\n",
        "    tot = len(train_data) // train_loader.batch_size\n",
        "    tot_val = len(val_data) // test_loader.batch_size\n",
        "    plot_steps = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss_total = 0.0\n",
        "        train_step = 0\n",
        "        # Training\n",
        "\n",
        "        model.train()\n",
        "        for entry in tqdm(train_loader, total=tot, position=0, leave=True):\n",
        "            loss = train(entry[0], entry[1], entry[2], entry[3], model, model_opt, scdl)\n",
        "            plot_steps += 1\n",
        "            train_step += 1\n",
        "            # if not math.isnan(loss) :\n",
        "            train_loss_total = train_loss_total + loss\n",
        "\n",
        "            train_loss = train_loss_total / train_step\n",
        "\n",
        "            if plot_steps % log_step == 0:\n",
        "                writer.add_scalar(\"Train Loss\", train_loss, plot_steps)\n",
        "\n",
        "            if (plot_steps % valid_step == 0) or (plot_steps >= num_train_steps - 1):\n",
        "                model.eval()\n",
        "                test_pred = []\n",
        "\n",
        "                for entry in tqdm(test_loader, total=tot_val, position=0, leave=True):\n",
        "                    loss_v, pred_v = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
        "                    test_pred.extend([pd for pd in pred_v])\n",
        "                val_acc = f1_score(gt_labels, test_pred, average='macro')\n",
        "                print(\"Validation F1: \" + str(val_acc))\n",
        "                writer.add_scalar(\"Val F1\", val_acc, plot_steps)\n",
        "                if val_acc > best_acc:\n",
        "                    torch.save(model.state_dict(), model_path + \"model_\" + base_model + \"_\" + lang + \"_easymix_hx\" + \".pth\")\n",
        "                    print(\"Model saved for step: \" + str(plot_steps))\n",
        "                    best_acc = val_acc\n",
        "\n",
        "                model.train()\n",
        "            writer.flush()\n",
        "\n",
        "        print('epoch: '+str(epoch))\n",
        "        print('total loss: '+str(train_loss_total/tot))\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ed2a7f13-faa6-4e64-857e-4b47eea70546",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed2a7f13-faa6-4e64-857e-4b47eea70546",
        "outputId": "08331d06-ba80-41ac-eb9b-c49fc65d649a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:02<00:00, 7295.25it/s]\n"
          ]
        }
      ],
      "source": [
        "#Load the data\n",
        "train_data = HateData(data_path=\"\", split='train', lang=lang,sep=',')\n",
        "val_data = HateData(data_path=\"\", split='test', lang=lang,sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c0410c5b-dd55-4523-9ce0-247538308727",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0410c5b-dd55-4523-9ce0-247538308727",
        "outputId": "a909c50a-c731-4805-fb1f-7e2652a85114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "480.0\n"
          ]
        }
      ],
      "source": [
        "#Load the Data\n",
        "BS = 16\n",
        "dataload = DataLoader(train_data, batch_size=BS, shuffle=True)\n",
        "dataload_val = DataLoader(val_data, batch_size=BS, shuffle=False)\n",
        "print((len(train_data)/16)//2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "47dbc8a3-d7fc-4ab9-8b82-ea95cac708d3",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47dbc8a3-d7fc-4ab9-8b82-ea95cac708d3",
        "outputId": "b7547fce-db77-4184-985c-6925bc44749c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#Load the classifier\n",
        "model = Classifier()\n",
        "model.load_state_dict(torch.load(\"Hx_robert_tws_easymix.pth\", map_location=device))\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "62b6aaed-5664-4b8f-b2b5-717104c11c16",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62b6aaed-5664-4b8f-b2b5-717104c11c16",
        "outputId": "af57a9fd-3851-431d-b232-38d28faea53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised optimizer and lr scheduler\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:13,  8.69it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6840707022848154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 168/961 [03:55<1:18:47,  5.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved for step: 168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.52it/s]                         \n",
            " 35%|███▍      | 336/961 [07:54<57:40,  5.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6775282637495245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.45it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6877716844094858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 504/961 [11:58<56:30,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved for step: 504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.46it/s]                         \n",
            " 70%|██████▉   | 672/961 [15:57<28:00,  5.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6840437620833747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.39it/s]                         \n",
            " 87%|████████▋ | 840/961 [19:55<11:17,  5.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6692532179671241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "962it [22:37,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "total loss: 2.0783050917437866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.45it/s]                         \n",
            "  5%|▍         | 46/961 [01:15<1:26:01,  5.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6052152106706453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.44it/s]                         \n",
            " 22%|██▏       | 214/961 [05:13<1:09:43,  5.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.665328760719032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.46it/s]                         \n",
            " 40%|███▉      | 382/961 [09:14<54:34,  5.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6540860158854666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.50it/s]                         \n",
            " 57%|█████▋    | 550/961 [13:07<38:52,  5.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.643694617951175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.47it/s]                         \n",
            " 75%|███████▍  | 718/961 [16:59<23:09,  5.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6680590880190228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.43it/s]                         \n",
            " 92%|█████████▏| 886/961 [20:57<07:03,  5.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6545025239464429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "962it [22:39,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "total loss: 2.0775161955532746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.39it/s]                         \n",
            " 10%|▉         | 92/961 [02:20<1:20:47,  5.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6405836526727932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.42it/s]                         \n",
            " 27%|██▋       | 260/961 [06:21<1:06:34,  5.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6755220000439048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.43it/s]                         \n",
            " 45%|████▍     | 428/961 [10:21<50:50,  5.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6548303590182684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.47it/s]                         \n",
            " 62%|██████▏   | 596/961 [14:20<35:18,  5.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6773727054747151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.46it/s]                         \n",
            " 80%|███████▉  | 764/961 [18:18<18:12,  5.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6567403345887719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.43it/s]                         \n",
            " 97%|█████████▋| 932/961 [22:17<02:41,  5.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6730584126678879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "962it [22:57,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2\n",
            "total loss: 2.0597660802033393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.45it/s]                         \n",
            " 14%|█▍        | 138/961 [03:19<1:19:10,  5.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6579882610969213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.46it/s]                         \n",
            " 32%|███▏      | 306/961 [07:17<1:02:26,  5.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6208350371499484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.45it/s]                         \n",
            " 49%|████▉     | 474/961 [11:16<45:44,  5.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6561780139637011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.46it/s]                         \n",
            " 67%|██████▋   | 642/961 [15:14<29:33,  5.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6419556287720098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.47it/s]                         \n",
            " 84%|████████▍ | 810/961 [19:13<13:59,  5.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.676826069805036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "962it [22:34,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 3\n",
            "total loss: 2.0324932628318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.47it/s]                         \n",
            "  2%|▏         | 16/961 [00:34<1:27:13,  5.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6742389325645807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.51it/s]                         \n",
            " 19%|█▉        | 184/961 [04:28<1:11:12,  5.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6570096423998519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.47it/s]                         \n",
            " 37%|███▋      | 352/961 [08:22<55:48,  5.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6680772051302989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.45it/s]                         \n",
            " 54%|█████▍    | 520/961 [12:16<41:10,  5.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6534109019052886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.42it/s]                         \n",
            " 72%|███████▏  | 688/961 [16:14<25:58,  5.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6480083513287557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.44it/s]                         \n",
            " 89%|████████▉ | 856/961 [20:12<10:18,  5.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6435578852827483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.47it/s]                         \n",
            "100%|██████████| 961/961 [22:46<00:00,  5.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6419149462296743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "121it [00:14,  8.43it/s]                         \n",
            "962it [23:01,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1: 0.6419149462296743\n",
            "epoch: 4\n",
            "total loss: 2.0051973611581584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Train the data\n",
        "trainIters(model, 5, dataload, dataload_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l827yYqHpGJ"
      },
      "source": [],
      "id": "9l827yYqHpGJ"
    },
    {
      "cell_type": "markdown",
      "id": "c1cfb1fe-2c3d-4f0d-b21d-21ce57faa3e5",
      "metadata": {
        "id": "c1cfb1fe-2c3d-4f0d-b21d-21ce57faa3e5"
      },
      "source": [
        "######################## TESTING ######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "19bbbce2-7488-4118-ba55-8a88b7d67a03",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19bbbce2-7488-4118-ba55-8a88b7d67a03",
        "outputId": "009f887b-c735-4428-d0a3-bc2f5a551142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Load pre trained Model\n",
        "model = Classifier()\n",
        "model.load_state_dict(torch.load(\"mnt/saved_models/test.pth\", map_location=device))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1bb602bc-5bc9-4da0-a92a-3798f2b91dca",
      "metadata": {
        "tags": [],
        "id": "1bb602bc-5bc9-4da0-a92a-3798f2b91dca"
      },
      "outputs": [],
      "source": [
        "#Load the Test Data\n",
        "test_data = HateData(data_path=\"\", split='test', lang=lang)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "1d3a48d8-f73b-41ff-98e9-4ba1734cb2cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d3a48d8-f73b-41ff-98e9-4ba1734cb2cc",
        "outputId": "3fa3bf66-0fd2-4b86-bd3f-eb786a032e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1924/1924 [00:28<00:00, 67.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss:  0.7562068308715219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_loss = []\n",
        "test_pred = []\n",
        "\n",
        "#Record the prediction result\n",
        "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + lang + \".txt\", \"w\")\n",
        "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
        "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
        "    test_loss.append(v_loss)\n",
        "    test_pred.append(v_pred)\n",
        "    wr.write(str(v_pred)+\"\\n\")\n",
        "test_loss = np.mean(test_loss)#.item()\n",
        "print(\"Test Loss: \", test_loss)\n",
        "wr.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "fb8b4197-be6d-44f1-b67d-89f8d3834b75",
      "metadata": {
        "id": "fb8b4197-be6d-44f1-b67d-89f8d3834b75"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(lang+\"_test.tsv\", sep='\\t', lineterminator='\\n')\n",
        "gt_labels = np.array(df_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9198ff7c",
      "metadata": {
        "id": "9198ff7c"
      },
      "outputs": [],
      "source": [
        "print(len(test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8d23d212-69ff-400c-a4e9-36173cae12a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d23d212-69ff-400c-a4e9-36173cae12a4",
        "outputId": "cc690f2b-8ae1-4596-d14e-c670ebf21356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6924    0.8606    0.7674       782\n",
            "           1     0.6398    0.4343    0.5174       548\n",
            "           2     0.7879    0.7694    0.7785       594\n",
            "\n",
            "    accuracy                         0.7110      1924\n",
            "   macro avg     0.7067    0.6881    0.6878      1924\n",
            "weighted avg     0.7069    0.7110    0.6996      1924\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(gt_labels, test_pred, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457197b8-1e12-4075-93ff-1202df2296eb",
      "metadata": {
        "id": "457197b8-1e12-4075-93ff-1202df2296eb"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(gt_labels, np.array(test_pred))"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-cu110.m89",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m89"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}