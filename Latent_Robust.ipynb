{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraies used in this IPYNB file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#torch lib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "#basic lib\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#sklearn Lib \n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#transformer lib Autotokenizer\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from transformers import BertModel, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "#NLTK lib and pandas\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import language_tool_python \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-requisite Function to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuda memory\n",
    "use_cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "base_model = 'twitter-xlm-roberta-base-sentiment'\n",
    "\n",
    "#model selection \n",
    "model_list = ['bert-base-uncased', 'bert-base-multilingual-uncased', 'google/muril-base-cased', 'xlm-roberta-base',\n",
    "              'ai4bharat/indic-bert','cardiffnlp/twitter-xlm-roberta-base','cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
    "              'cardiffnlp/twitter-roberta-base', 'cardiffnlp/twitter-roberta-base-sentiment',\n",
    "              'cardiffnlp/twitter-roberta-base-hate', 'roberta-base']\n",
    "\n",
    "#model path \n",
    "model_path = 'mnt/saved_models/'\n",
    "\n",
    "#result are saaved in this location \n",
    "results_path = 'mnt/saved_results/'\n",
    "\n",
    "#Data augumentation \n",
    "class HateData(Dataset):\n",
    "    def __init__(self, data_path, split='train', lang='bengali', aug_prob=0.2, flip_prob=0.5):\n",
    "        self.split = split\n",
    "        self.data = pd.read_csv(data_path + split + lang + \".tsv\", sep='\\t', lineterminator='\\n') \n",
    "        if self.split == 'train':\n",
    "            self.label2data = {0:[], 1:[], 2:[]}\n",
    "            for i in tqdm(range(len(self.data))):\n",
    "                row = self.data.iloc[i]\n",
    "                self.label2data[row[label_idx]].append(row[text_idx])\n",
    "            self.aug_prob = aug_prob\n",
    "            self.flip_prob = flip_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        data = self.data.iloc[index]\n",
    "        labels = data[label_idx]\n",
    "        text = data[text_idx]\n",
    "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        input_ids = inputs['input_ids']\n",
    "        token_type_ids = np.zeros(MAX_SEQ_LEN)\n",
    "        attn_mask = inputs['attention_mask']\n",
    "        aug_text = text  \n",
    "        labels_aug = labels\n",
    "        \n",
    "        if self.split == 'train' and labels == 1:\n",
    "            if np.random.uniform() < self.aug_prob:\n",
    "                aug_text = np.random.choice(self.label2data[0])\n",
    "         \n",
    "                if np.random.uniform() < self.flip_prob:\n",
    "                    aug_text = aug_text + \" [SEP] \" + text\n",
    "                else:\n",
    "                    aug_text = text + \" [SEP] \" + aug_text \n",
    "            labels_aug = 1\n",
    "      \n",
    "        inputs_aug = tokenizer(aug_text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        input_ids_aug = inputs_aug['input_ids']\n",
    "        token_type_ids_aug = np.zeros(MAX_SEQ_LEN)\n",
    "        attn_mask_aug = inputs_aug['attention_mask']\n",
    "\n",
    "        input_ids = torch.tensor(np.vstack([input_ids, input_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        token_type_ids = torch.tensor(np.vstack([token_type_ids, token_type_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        attn_mask = torch.tensor(np.vstack([attn_mask, attn_mask_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        labels = torch.tensor(np.vstack([labels, labels_aug]), dtype=torch.long).view(2)\n",
    "\n",
    "        return input_ids, attn_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#data classifier \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        H1, H2, num_class = 768, 128, 2\n",
    "        self.bert = AutoModel.from_pretrained(model_list[model_choice])\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(H1, H2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H2, H2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H2, num_class)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):  \n",
    "        outputs = self.bert(input_ids, attn_mask)\n",
    "        cls_emb = outputs.pooler_output \n",
    "        logits = self.clf(cls_emb)\n",
    "        return logits\n",
    "    \n",
    "#evaluate fuction \n",
    "def evaluate(input_ids, attn_mask, token_type_ids, label, model, mode='train'):\n",
    "   \n",
    "    batch_size = input_ids.shape[0]\n",
    "    seq_len = input_ids.shape[1]\n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            label = label.to(device)\n",
    "        \n",
    "        logits = model(input_ids[:,0,:], attn_mask[:,0,:], token_type_ids[:,0,:])\n",
    "        loss = loss_fn(logits, label[:,0])\n",
    "        \n",
    "        if mode == 'train':\n",
    "            return float(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        return float(loss.item()), preds.cpu().numpy()\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested Model with 1000 samples for Hate Explain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:24<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.43809041582606734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8369    0.8771    0.8565       667\n",
      "           1     0.7276    0.6577    0.6909       333\n",
      "\n",
      "    accuracy                         0.8040      1000\n",
      "   macro avg     0.7822    0.7674    0.7737      1000\n",
      "weighted avg     0.8005    0.8040    0.8013      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"data/Latenthatred/latent_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"class\"][i])\n",
    "\n",
    "test_sam['post']=sam1\n",
    "test_sam['label']=sam2\n",
    "test_sam.to_csv(\"hxsam_test.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Latent_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym Replacement\n",
    "#### All the words in the sentences are replaced by their synonyms while they're not stopwords, when their length is more than 3 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:36<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.5136432139184326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8080    0.8771    0.8411       667\n",
      "           1     0.7029    0.5826    0.6371       333\n",
      "\n",
      "    accuracy                         0.7790      1000\n",
      "   macro avg     0.7555    0.7298    0.7391      1000\n",
      "weighted avg     0.7730    0.7790    0.7732      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=pd.read_csv(\"data/latenthatred/latent_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"class\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    for i in l:\n",
    "        if i in stop_words:\n",
    "            sen.append(i)\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                if len(i)<=3:\n",
    "                    sen.append(i)\n",
    "                else:\n",
    "                    j=wordnet.synsets(i)\n",
    "                    lt=j[0].lemmas()[0].name()\n",
    "                    if i !=lt:\n",
    "                        sen.append(lt)\n",
    "                    else:\n",
    "                        sen.append(i)\n",
    "            except:\n",
    "                sen.append(i)\n",
    "    post.append(' '.join(sen))\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/latent_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charcter swaping\n",
    "#### All the words in the sentences are swaped their charcters while they're not stopwords, when their length is more than 3 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:17<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.5482584201395512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7869    0.9025    0.8408       667\n",
      "           1     0.7234    0.5105    0.5986       333\n",
      "\n",
      "    accuracy                         0.7720      1000\n",
      "   macro avg     0.7552    0.7065    0.7197      1000\n",
      "weighted avg     0.7658    0.7720    0.7601      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "data=pd.read_csv(\"data/latenthatred/latent_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"class\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    count=0\n",
    "    for i in l:\n",
    "        if i in stop_words:\n",
    "            sen.append(i)\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                if len(i)<=3:\n",
    "                    sen.append(i)\n",
    "                else:\n",
    "                    if count==2:\n",
    "                        sen.append(i)\n",
    "                    else:\n",
    "                        t=list(i)\n",
    "                        t[2],t[3]=t[3],t[2]\n",
    "                        r=\"\".join(t)\n",
    "                        count=count+1\n",
    "                        sen.append(r)\n",
    "            except:\n",
    "                sen.append(i)\n",
    "    post.append(' '.join(sen))\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Latent_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation last charcter of the word to next word in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:15<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.5144728650208562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8164    0.8666    0.8407       667\n",
      "           1     0.6952    0.6096    0.6496       333\n",
      "\n",
      "    accuracy                         0.7810      1000\n",
      "   macro avg     0.7558    0.7381    0.7452      1000\n",
      "weighted avg     0.7760    0.7810    0.7771      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "data=pd.read_csv(\"data/latenthatred/latent_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"class\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    count=0\n",
    "    word=\"\"\n",
    "    for i in l:\n",
    "        if word!=\"\":\n",
    "            i=word+i\n",
    "            word=\"\"\n",
    "        else:\n",
    "            pass\n",
    "        if i in stop_words:\n",
    "            sen.append(i)\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                if len(i)<=3:\n",
    "                    sen.append(i)\n",
    "                else:\n",
    "                    if count==1:\n",
    "                        sen.append(i)\n",
    "                    else:\n",
    "                        t=list(i)\n",
    "                        word=t.pop()\n",
    "                        #print(word)\n",
    "                        r=\"\".join(t)\n",
    "                        count=count+1\n",
    "                        sen.append(r)\n",
    "            except:\n",
    "                sen.append(i)\n",
    "    post.append(' '.join(sen))\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Latent_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping few words from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  post\n",
      "0    dylan random kori white media and he by is pen...\n",
      "1    read s iran the the of holocaust more question...\n",
      "2    racist make entire sense doesn why they and wh...\n",
      "3    s disrespecting t loser kaepernick rally kraep...\n",
      "4    and that all enemies states must be united all...\n",
      "..                                                 ...\n",
      "995  spencer better spencer know white should richa...\n",
      "996  opposing speech hate foe for of of blasphemy a...\n",
      "997  supporters conservatives debate their trumps w...\n",
      "998  for thing people the prison guards good jobs t...\n",
      "999  are to person 6 person 32 blacks are white to ...\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:26<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.6366159313134849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7541    0.9061    0.8231       660\n",
      "           1     0.7005    0.4265    0.5302       340\n",
      "\n",
      "    accuracy                         0.7430      1000\n",
      "   macro avg     0.7273    0.6663    0.6766      1000\n",
      "weighted avg     0.7359    0.7430    0.7235      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import language_tool_python \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "data=pd.read_csv(\"data/Latenthatred/latent_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(500,1500):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"class\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    if len(l)>3:\n",
    "        sen=[]\n",
    "        count=0\n",
    "        word=\"\"\n",
    "        for i in l:\n",
    "            sen.append(i)\n",
    "        random.shuffle(sen)\n",
    "        sen.pop()\n",
    "        random.shuffle(sen)\n",
    "        sen.pop()\n",
    "        post.append(' '.join(sen))\n",
    "        post\n",
    "    else:\n",
    "        post.append(sam1[i])\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "print(test_sam)\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/latent_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [05:26<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.5633190030883998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7799    0.8981    0.8348       667\n",
      "           1     0.7069    0.4925    0.5805       333\n",
      "\n",
      "    accuracy                         0.7630      1000\n",
      "   macro avg     0.7434    0.6953    0.7077      1000\n",
      "weighted avg     0.7556    0.7630    0.7502      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "data=pd.read_csv(\"data/latenthatred/latent_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"class\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    count=0\n",
    "    word=\"\"\n",
    "    for i in l:\n",
    "        sen.append(i)\n",
    "    random.shuffle(sen)\n",
    "    random.shuffle(sen)\n",
    "    post.append(' '.join(sen))\n",
    "    post\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Latent_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Special Charcters to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "grammer = language_tool_python.LanguageTool('en-US')\n",
    "from string import punctuation\n",
    "pn=[punctuation]\n",
    "char=random.choice(pn)\n",
    "\n",
    "data=pd.read_csv(\"data/latenthatred/latent_test.tsv\", sep='\\t')\n",
    "data[\"post\"]\n",
    "sam1=[]\n",
    "sam2=[]\n",
    "post=[]\n",
    "\n",
    "test_sam=pd.DataFrame()\n",
    "for i in range(1000):\n",
    "    sam1.append(data[\"post\"][i])\n",
    "    sam2.append(data[\"class\"][i])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "for i in range(len(sam1)):\n",
    "    \n",
    "    l=sam1[i].split()\n",
    "    sen=[]\n",
    "    count=0\n",
    "    word=\"\"\n",
    "    for i in l:\n",
    "        sen.append(i+char)\n",
    "    post.append(' '.join(sen))\n",
    "    post\n",
    "\n",
    "\n",
    "test_sam['post']=post\n",
    "test_sam['label']=sam2\n",
    "\n",
    "test_sam.to_csv(\"hxsam_test2.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "model_choice = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_list[model_choice])\n",
    "#Load pre trained Model\n",
    "model = Classifier()\n",
    "model.load_state_dict(torch.load(\"mnt/saved_models/Latent_tws.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "label_idx = 1\n",
    "MAX_SEQ_LEN = 128\n",
    "text_idx = 0\n",
    "\n",
    "test_data = HateData(data_path=\"\", split='', lang=\"hxsam_test2\")\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = []\n",
    "test_pred = []\n",
    "\n",
    "#Record the prediction result  \n",
    "wr = open(results_path + \"test_prediction_\" + base_model + \"_\" + \"Hx\" + \".txt\", \"w\")    \n",
    "for entry in tqdm(test_loader, total=len(test_data)//test_loader.batch_size, position=0, leave=True):\n",
    "    v_loss, v_pred = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "    test_loss.append(v_loss)\n",
    "    test_pred.append(v_pred)\n",
    "    wr.write(str(v_pred)+\"\\n\")\n",
    "test_loss = np.mean(test_loss)#.item()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "wr.close()\n",
    "\n",
    "df_test = pd.read_csv(\"data/multilingual/test_\"+\"Hx\"+\".tsv\", sep='\\t', lineterminator='\\n')\n",
    "gt_labels = test_sam[\"label\"]\n",
    "\n",
    "print(classification_report(gt_labels, test_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
